## После проведения разведочного анализа данных (EDA) для нашего проекта в основе которого лежит RAG (Retrieval-Augmented Generation), можно сделать следующие выводы:
## 1) Качество данных:
Пропущенные значения: Обнаружено небольшое количество пропущенных значений в столбце источник, в количестве 3 штук, которые в перспективе скорее всего придется просто удалить, так как оценить качество ответа на вопрос, которого нет - явно невозможно.
## 2) Распределение длины текста:
Большинство текстов имеют длину от 200 до 1400 токенов,что явно превышает окно контекста нашего эмбедера. Это может вполне отразиться на качестве retrieval-части проекта, поэтому в будущем есть смысл провести эксперименты с моделями имеющими большую длину контекста.
## 3) Визуализация:
Для визуализации данных были построены облака точек для контекста,вопросов и ответов.Так же был построен график наиболее встречаемых слов в questions, answers, context,и
построена визуализация эмбеддингов с помощью метода t-SNE.
## Более подробно про Гистограммы токенизированных questions, context, answer:
## Гистограмма контекстов

![image](https://github.com/user-attachments/assets/3d0c9628-4527-44cf-9cc4-d9cb8bbe0d4e)

Видно, что тексты будут сильно обрубаться токенизатором, что в последствии может сказаться на точность RAG.
## Гистограмма Вопросов
![image](https://github.com/user-attachments/assets/c2abcd69-ede4-4935-8b04-bd9151ac9235)

Из гистограммы видно,что вопросы должны без проблем эмбеддится base-line моделью. Тут должно быть все хорошо
## Гистограмма Ответов:
![image](https://github.com/user-attachments/assets/5d7c82e5-d22f-4d6e-b68c-cc5a353a75bd)

Ответы так же должны без проблем будут эмбеддится base-line моделью. Тут должно так же быть все хорошо.

## Вывод:
Учитывая гистограмму контекстов ,вероятно придется использовать эмбедеры с большим окном контекста, или уже нарезать имеющиеся чанки на более мелкие части

# Первичный анализ retrieval-части проекта.
В качестве baseline'а будет использоваться all-MiniLM-L6-v2, Ссылка на hf:https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 
Для первых двадцати пар вопрос/контекст были получены эмбеддинги по первым 256 токенам, и посчитано коссинусное расстояние между ними всеми. Построен Heatmap (по диагонали находятся релевантный вопросу контекст):
![image](https://github.com/user-attachments/assets/c020c3cb-4ea3-472b-b1f3-88d07098f4fc)

Позже подобная операция была проведина для всей выборки, была посчитано текущая точность эмбеддира, метрика accuracy( Является ли top-1 контекст по коссиносному расстоянию - релевантным).
Таким образом точность "по топ-1 контексту" составила 75% - не лучший результат, стоит явно производить эксперименты с chunking или пробовать модели с большим окном контекста.
